{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Create a daily NAO index using the CPC methodology \n",
    "=====\n",
    "\n",
    "Author: [Ray Bell](https://github.com/raybellwaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load python packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import holoviews as hv\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "hv.notebook_extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the time period of the [SubX](http://iridl.ldeo.columbia.edu/SOURCES/.Models/.SubX/) hindcast: 1999-2016.\n",
    "For a saninty check make sure that 1999-2016 is indeed the hindcast period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_data = xr.open_dataset('http://iridl.ldeo.columbia.edu/SOURCES/.Models'\\\n",
    "                              '/.SubX/.RSMAS/.CCSM4/.hindcast/.zg/dods')\n",
    "print(remote_data.coords['S'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used here is geopotential height at 500 hPa ($Z_{500}$) from ERA-Interim. This data was simply copied from the University of Reading archive. You can however use the [API](https://software.ecmwf.int/wiki/display/CKB/How+to+download+data+via+the+ECMWF+WebAPI) to download the data from ECMWF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two files exists in this directory `ERAI_z500_monthly_1999-2016.nc` (43 Mb) and `b.nc`. `ERAI_z500_monthly_1999-2016.nc` is the monthly mean of $Z_{500}$ January 1996 - December 2016. `b.nc` is daily $Z_{500}$ January 1st 1996 - December 31st 2016. The data has been interpolated to 1° to match the SubX data (360x181) and cut to 20°N-90°N for the analysis. The reference scripts for the data processing beforehand are in the directory `pre_proc`. DO THIS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPC's methodology for the daily NAO index can be found [here](http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/history/method.shtml), with some more information [here](http://www.cpc.ncep.noaa.gov/data/teledoc/telepatcalc.shtml). The procedure is based on [Barston and Livezey (1987)](https://journals.ametsoc.org/doi/pdf/10.1175/1520-0493%281987%29115%3C1083%3ACSAPOL%3E2.0.CO%3B2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My understanding of the methodology is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the region of 20°N-90°N with standardized $Z_{500}$ anomalies.\n",
    "\n",
    "2. Calculate the 10 leading EOFs for each calendar month centred on that month with a window of 3 months. e.g. February is based on January-February-March (JFM). Decisions have to be made for January and December: whether to just use the two-month window or leave out the season. (It shouldn't really matter).\n",
    "\n",
    "3. Rotate the EOFs using the varimax methodology.\n",
    "\n",
    "4. Linearly interpolate the monthly spatial pattern to the day in question. e.g. February 1$^{st}$ will be linearly interpolated from the DJF pattern (think of the pattern defined mid-way through the mid-month: e.g. January 15$^{th}$) and the JFM pattern (February 15$^{th}$).\n",
    "\n",
    "5. Least squared regression approach for daily data? (e.g. last paragraph in the NAO/PNA section [here](http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/history/method.shtml) (I don't understand this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try creating a daily index for one day e.g. February 15$^{th}$ 2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to obtain the NAO pattern from the monthly data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcualte the 10 leading EOFs for all JFMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use xarray's [rolling mean](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.rolling.html) to do the seasonal averages. Then slice it to get all the JFM's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = xr.open_dataarray('ERAI_z500_monthly_1999-2016.nc')\n",
    "print(da)\n",
    "\n",
    "sm = da.rolling(time=3).mean().dropna('time')\n",
    "# Make note that time is now given as the last month in the window e.g. JFM has time 03-16T09 (MM-DDTHH)\n",
    "\n",
    "# Use numpy's slice index to get all the JFM's\n",
    "jfm = sm[0::12,:,:]\n",
    "# Check that all the JFM 1999-2016 were correctly sliced \n",
    "print(jfm.coords['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate standardized anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jfm_sa = (jfm - jfm.mean(dim=('time'))) / (jfm - jfm.mean(dim=('time'))).std(dim=('time'))\n",
    "print(jfm_sa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing the EOF apply a weighting as cosine of the latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jfm_sa.values\n",
    "wgts = np.sqrt(np.cos(np.deg2rad(jfm_sa.coords['lat'].values)).clip(0., 1.))[..., np.newaxis]\n",
    "weights = np.broadcast_arrays(data[0:1], wgts)[1][0]\n",
    "data = data * weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the 10 leading EOF modes. For a basic understanding of EOFs I recommending reading Hannachi's EOF primer [here](http://www.met.rdg.ac.uk/~han/Monitor/eofprimer.pdf) and his paper [here](https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/joc.1499). This code is also adapted from Dawson's [eof package](https://github.com/ajdawson/eofs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to be (time, space)\n",
    "records = len(jfm_sa.coords['time'])\n",
    "originalshape = data.shape[1:]\n",
    "channels = np.prod(originalshape)\n",
    "data_flat = data.reshape([records, channels])\n",
    "print(np.shape(data_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the singular value decomposition\n",
    "A, Lh, E = np.linalg.svd(data_flat, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the eigenvalues and normalize by N-1\n",
    "L = (Lh * Lh) / (float(records - 1))\n",
    "\n",
    "# Remove the scaling on the principal component time-series that is\n",
    "# implicitily introduced by using SVD instead of eigen-decomposition.\n",
    "# The PCs may be re-scaled later if required.\n",
    "P = A * Lh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the 10 dominant EOFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Principal Components\n",
    "npcs = 10\n",
    "pcs = P[:, 0:npcs] / np.sqrt(L[0:npcs])\n",
    "# Put it in a DataArray\n",
    "pcs_da = xr.DataArray(pcs, coords=[jfm_sa.coords['time'], range(pcs.shape[1])],\n",
    "                      dims=['time', 'mode'], name='pcs')\n",
    "print(pcs_da)\n",
    "\n",
    "# Calculate the eofs\n",
    "flat_eofs = E[0:npcs, :].copy()\n",
    "eofs = flat_eofs / np.sqrt(L[0:npcs])[:, np.newaxis]\n",
    "print(np.shape(eofs))\n",
    "# Return the original shape\n",
    "eofs2d = eofs.reshape((npcs,) + originalshape)\n",
    "# Put it in a DataArray\n",
    "eofs2d_da = xr.DataArray(eofs2d, coords=[range(pcs.shape[1]), jfm_sa.coords['lat'], jfm_sa.coords['lon']],\n",
    "                         dims=['mode', 'lat', 'lon'], name='eofs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the 1$^{st}$ EOF as covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the input by the weighting\n",
    "data = data / weights\n",
    "\n",
    "out_shape = (npcs,) + originalshape\n",
    "data_flat = data.reshape([records, channels])\n",
    "pcs_flat = pcs.reshape([records, npcs])\n",
    "\n",
    "# Divisor\n",
    "div = np.float64(pcs_flat.shape[0] - float(records - 1))\n",
    "cov = (np.dot(data_flat.T, pcs_flat).T / div).reshape(out_shape)\n",
    "# Put into DataArray\n",
    "cov_da = xr.DataArray(cov, coords=[range(pcs.shape[1]), jfm_sa.coords['lat'], jfm_sa.coords['lon']],\n",
    "                         dims=['mode', 'lat', 'lon'], name='z500')\n",
    "print(cov_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_da.isel(mode=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the plot nicer using [cartopy](https://github.com/SciTools/cartopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection=ccrs.Orthographic(0, 90))\n",
    "ax.coastlines()\n",
    "ax.set_global()\n",
    "cov_da.isel(mode=0).plot.contourf(ax=ax, transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the variance explained by each mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varexpl = (L[0:npcs] / L.sum()) * 100\n",
    "print(varexpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the other modes of variability using [geoviews](https://github.com/ioam/geoviews). Just move the slider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Image [projection=ccrs.Orthographic(0, 90) colorbar=False fig_size=200] (cmap='RdBu_r') Overlay [xaxis=None yaxis=None]\n",
    "dataset = gv.Dataset(cov_da, kdims=['mode', 'lon', 'lat'])\n",
    "dataset.to(gv.Image, ['lon', 'lat']) * gf.coastline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some known patterns here and others may be spurious\n",
    "- 1st Looks a bit like the [Artic Oscillation](http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/loading.html)\n",
    "- 2nd Looks like the [Pacific North American](http://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/pna_loading.html)\n",
    "- 3rd. Is possibly the [NAO](http://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/nao_loading.html) but there is a lot of activity outside of the North Atlantic. Perhaps performing the REOF will reduce the noise for this variable outside of the North Atlantic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of rotating the eofs is to maximise the sum of the variances so the coefficients will either be large or near-zero. You can read about it more in [Richman 1986](https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/joc.3370060305). The most common method is the varimax rotation. There is an answer for how to do with numpy [here](https://stackoverflow.com/questions/17628589/perform-varimax-rotation-in-python-using-numpy), *bmcmenamin* has a fa_kit where is does the rotation [here](https://github.com/bmcmenamin/fa_kit/blob/master/fa_kit/rotation.py) based on a script by *rossfadely* [here](https://github.com/rossfadely/consomme/blob/master/consomme/rotate_factor.py). Dawson also has the rotation as WIP [here](https://github.com/ajdawson/eofs/blob/experimental-rotation/lib/eofs/experimental/rotation/kernels.py). I hope to build on that package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-10 # Tolerance value used to determine convergence of the rotation algorithm\n",
    "itermax = 1000 # Maximum number of iterations \n",
    "\n",
    "# Apply kaiser row normalization\n",
    "scale = np.sqrt((eofs ** 2).sum(axis=0))\n",
    "eofs_norm = eofs / scale\n",
    "\n",
    "rotation = np.eye(npcs, dtype=eofs_norm.dtype) # Initialize\n",
    "delta = 0.\n",
    "for i in range(itermax):\n",
    "    z = np.dot(eofs_norm.T, rotation)\n",
    "    b = np.dot(eofs_norm,\n",
    "               z ** 3 - np.dot(z, np.diag((z ** 2).sum(axis=0)) / channels))\n",
    "    u, s, v = np.linalg.svd(b)\n",
    "    rotation = np.dot(u, v)\n",
    "    delta_previous = delta\n",
    "    delta = s.sum()\n",
    "    if delta < delta_previous * (1. + eps):\n",
    "        break\n",
    "reofs = np.dot(eofs_norm.T, rotation).T\n",
    "reofs = reofs * scale\n",
    "print(np.shape(reofs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute variances of the rotated EOFs\n",
    "reofs_var = (reofs ** 2).sum(axis=1)\n",
    "\n",
    "reofs2d = reofs.reshape((npcs,) + originalshape)\n",
    "#print(np.shape(reofs2d))\n",
    "\n",
    "nspace = np.prod(channels)\n",
    "ev = eofs.reshape([npcs, nspace])\n",
    "print(np.shape(ev))\n",
    "#ev_nm = ev[:, nmi]\n",
    "#field = eofs.reshape((npcs,) + originalshape)\n",
    "#print(np.shape(field))\n",
    "#print(np.shape(reofs2d))\n",
    "\n",
    "rpcs = np.dot(ev, reofs.T)\n",
    "# Should be (time(18), mode(10)) but it's (10, 10) eofs oroginally is 18 x channels (this packages computes eofs as the len of time)\n",
    "# However, eofs is cut to 10 to include the 10 dominant ones beforehand to returns (10, 10)\n",
    "print(np.shape(rpcs))\n",
    "#print(rpcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter a start-time an end-time for which to calculate the daily NAO index\n",
    "e.g. lets look at the large negativate NAO of winter 2008/2009:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = '2009-02-01-T12:00:00'\n",
    "etime = '2009-02-14-T12:00:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extend for the winter months (DJF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = '2008-12-01-T12:00:00'\n",
    "etime = '2009-02-28-T12:00:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do for all winters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
