{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Create a daily NAO index using the CPC methodology \n",
    "=====\n",
    "\n",
    "Author: [Ray Bell](https://github.com/raybellwaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the time period of the [SubX](http://iridl.ldeo.columbia.edu/SOURCES/.Models/.SubX/) hindcast: 1999-2016.\n",
    "For a saninty check make sure that 1999-2016 is indeed the hindcast period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'S' (S: 6569)>\n",
      "array(['1999-01-07T00:00:00.000000000', '1999-01-08T00:00:00.000000000',\n",
      "       '1999-01-09T00:00:00.000000000', ..., '2016-12-29T00:00:00.000000000',\n",
      "       '2016-12-30T00:00:00.000000000', '2016-12-31T00:00:00.000000000'],\n",
      "      dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * S        (S) datetime64[ns] 1999-01-07 1999-01-08 1999-01-09 1999-01-10 ...\n",
      "Attributes:\n",
      "    pointwidth:     0.0\n",
      "    long_name:      Start Time\n",
      "    standard_name:  forecast_reference_time\n",
      "    gridtype:       0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "remote_data = xr.open_dataset('http://iridl.ldeo.columbia.edu/SOURCES/.Models'\\\n",
    "                              '/.SubX/.RSMAS/.CCSM4/.hindcast/.zg/dods')\n",
    "print(remote_data.coords['S'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used here is geopotential height at 500 hPa ($Z_{500}$) from ERA-Interim. This data was simply copied from the University of Reading archive. You can however use the [API](https://software.ecmwf.int/wiki/display/CKB/How+to+download+data+via+the+ECMWF+WebAPI) to download the data from ECMWF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two files exists in this directory `ERAI_z500_monthly_1999-2016.nc` (43 Mb) and `b.nc`. `ERAI_z500_monthly_1999-2016.nc` is the monthly mean of $Z_{500}$ January 1996 - December 2016. `b.nc` is daily $Z_{500}$ January 1st 1996 - December 31st 2016. The data has been interpolated to 1° to match the SubX data (360x181) and cut to 20°N-90°N for the analysis. The reference scripts for the data processing beforehand are in the directory `pre_proc`. DO THIS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPC's methodology for the daily NAO index can be found [here](http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/history/method.shtml), with some more information [here](http://www.cpc.ncep.noaa.gov/data/teledoc/telepatcalc.shtml). The procedure is based on [Barston and Livezey (1987)](https://journals.ametsoc.org/doi/pdf/10.1175/1520-0493%281987%29115%3C1083%3ACSAPOL%3E2.0.CO%3B2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My understanding of the methodology is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the region of 20°N-90°N with standardized $Z_{500}$ anomalies.\n",
    "\n",
    "2. Calculate the 10 leading EOFs for each calendar month centred on that month with a window of 3 months. e.g. February is based on January-February-March (JFM). Decisions have to be made for January and December: whether to just use the two-month window or leave out the season. (It shouldn't really matter).\n",
    "\n",
    "3. Rotate the EOFs using the varimax methodology.\n",
    "\n",
    "4. Linearly interpolate the monthly spatial pattern to the day in question. e.g. February 1$^{st}$ will be linearly interpolated from the DJF pattern (think of the pattern defined mid-way through the mid-month: e.g. January 15$^{th}$) and the JFM pattern (February 15$^{th}$).\n",
    "\n",
    "5. Least squared regression approach for daily data? (e.g. last paragraph in the NAO/PNA section [here](http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/history/method.shtml) (I don't understand this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try creating a daily index for one day e.g. February 15$^{th}$ 2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to obtain the NAO pattern from the monthly data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calcualte the 10 leading EOFs for all JFMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use xarray's [rolling mean](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.rolling.html) to do the seasonal averages. Then slice it to get all the JFM's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'gph500hPa' (time: 216, lat: 71, lon: 360)>\n",
      "[5520960 values with dtype=float64]\n",
      "Coordinates:\n",
      "  * lon      (lon) float64 0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 ...\n",
      "  * time     (time) datetime64[ns] 1999-01-16T09:00:00 1999-02-14T21:00:00 ...\n",
      "  * lat      (lat) float64 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 ...\n",
      "Attributes:\n",
      "    standard_name:  geopotential_height\n",
      "    long_name:      Geopotential Height\n",
      "    units:          meters\n",
      "    name:           Z\n",
      "    source:         GRIB data\n",
      "    time:           00:00\n",
      "    title:          Geopotential\n",
      "\n",
      "<xarray.DataArray (time: 214, lat: 71, lon: 360)>\n",
      "array([[[5809.654566, 5810.972497, ..., 5807.253832, 5808.479121],\n",
      "        [5796.711947, 5797.982469, ..., 5794.312084, 5795.529759],\n",
      "        ...,\n",
      "        [5118.6511  , 5118.760763, ..., 5118.42919 , 5118.540497],\n",
      "        [5117.234469, 5117.234469, ..., 5117.234469, 5117.234469]],\n",
      "\n",
      "       [[5827.259486, 5829.044683, ..., 5823.829739, 5825.577047],\n",
      "        [5816.223761, 5818.059066, ..., 5812.517349, 5814.371712],\n",
      "        ...,\n",
      "        [5163.825832, 5163.888646, ..., 5163.701262, 5163.763469],\n",
      "        [5164.121632, 5164.121632, ..., 5164.121632, 5164.121632]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[5881.244269, 5881.379357, ..., 5881.073584, 5881.144899],\n",
      "        [5879.551452, 5879.721973, ..., 5879.145149, 5879.346754],\n",
      "        ...,\n",
      "        [5262.365161, 5262.558775, ..., 5261.973679, 5262.170064],\n",
      "        [5260.289036, 5260.289036, ..., 5260.289036, 5260.289036]],\n",
      "\n",
      "       [[5855.761029, 5855.943348, ..., 5855.495893, 5855.633048],\n",
      "        [5850.00605 , 5850.152037, ..., 5849.706247, 5849.869906],\n",
      "        ...,\n",
      "        [5185.342547, 5185.476946, ..., 5185.072284, 5185.207704],\n",
      "        [5184.807199, 5184.807199, ..., 5184.807199, 5184.807199]]])\n",
      "Coordinates:\n",
      "  * lon      (lon) float64 0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 ...\n",
      "  * time     (time) datetime64[ns] 1999-03-16T09:00:00 1999-04-15T21:00:00 ...\n",
      "  * lat      (lat) float64 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 ...\n",
      "\n",
      "<xarray.DataArray (time: 18, lat: 71, lon: 360)>\n",
      "array([[[5809.654566, 5810.972497, ..., 5807.253832, 5808.479121],\n",
      "        [5796.711947, 5797.982469, ..., 5794.312084, 5795.529759],\n",
      "        ...,\n",
      "        [5118.6511  , 5118.760763, ..., 5118.42919 , 5118.540497],\n",
      "        [5117.234469, 5117.234469, ..., 5117.234469, 5117.234469]],\n",
      "\n",
      "       [[5816.302132, 5816.262093, ..., 5816.237415, 5816.359145],\n",
      "        [5807.43646 , 5807.202681, ..., 5807.540204, 5807.551429],\n",
      "        ...,\n",
      "        [5099.623758, 5099.697753, ..., 5099.478418, 5099.550549],\n",
      "        [5101.422329, 5101.422329, ..., 5101.422329, 5101.422329]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[5827.928719, 5828.37923 , ..., 5826.960467, 5827.509476],\n",
      "        [5818.05497 , 5818.331891, ..., 5817.229005, 5817.708798],\n",
      "        ...,\n",
      "        [5050.022383, 5049.984317, ..., 5050.098918, 5050.060588],\n",
      "        [5050.917199, 5050.917199, ..., 5050.917199, 5050.917199]],\n",
      "\n",
      "       [[5855.689468, 5856.374055, ..., 5854.332708, 5855.045908],\n",
      "        [5846.763385, 5847.241948, ..., 5845.935048, 5846.363564],\n",
      "        ...,\n",
      "        [5161.995147, 5162.029191, ..., 5161.931387, 5161.96265 ],\n",
      "        [5164.398867, 5164.398867, ..., 5164.398867, 5164.398867]]])\n",
      "Coordinates:\n",
      "  * lon      (lon) float64 0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 ...\n",
      "  * time     (time) datetime64[ns] 1999-03-16T09:00:00 2000-03-16T09:00:00 ...\n",
      "  * lat      (lat) float64 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 ...\n",
      "<xarray.DataArray 'time' (time: 18)>\n",
      "array(['1999-03-16T09:00:00.000000000', '2000-03-16T09:00:00.000000000',\n",
      "       '2001-03-16T09:00:00.000000000', '2002-03-16T09:00:00.000000000',\n",
      "       '2003-03-16T09:00:00.000000000', '2004-03-16T09:00:00.000000000',\n",
      "       '2005-03-16T09:00:00.000000000', '2006-03-16T09:00:00.000000000',\n",
      "       '2007-03-16T09:00:00.000000000', '2008-03-16T09:00:00.000000000',\n",
      "       '2009-03-16T09:00:00.000000000', '2010-03-16T09:00:00.000000000',\n",
      "       '2011-03-16T09:00:00.000000000', '2012-03-16T12:00:00.000000000',\n",
      "       '2013-03-16T12:00:00.000000000', '2014-03-16T12:00:00.000000000',\n",
      "       '2015-03-16T12:00:00.000000000', '2016-03-16T12:00:00.000000000'],\n",
      "      dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 1999-03-16T09:00:00 2000-03-16T09:00:00 ...\n",
      "Attributes:\n",
      "    standard_name:  time\n",
      "    long_name:      t\n"
     ]
    }
   ],
   "source": [
    "da = xr.open_dataarray('ERAI_z500_monthly_1999-2016.nc')\n",
    "print(da)\n",
    "\n",
    "sm = da.rolling(time=3).mean().dropna('time')\n",
    "# Make note that time is now given as the last month in the window e.g. JFM has time 03-16T09 (MM-DDTHH)\n",
    "print('')\n",
    "print(sm)\n",
    "\n",
    "# Use numpy's slice index to get all the JFM's\n",
    "jfm = sm[0::12,:,:]\n",
    "print('')\n",
    "print(jfm)\n",
    "# Check that all the JFM 1999-2016 were correctly sliced \n",
    "print(jfm.coords['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate standardized anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray (time: 18, lat: 71, lon: 360)>\n",
      "array([[[-1.434221, -1.399658, ..., -1.488877, -1.463079],\n",
      "        [-1.575146, -1.546949, ..., -1.617144, -1.596318],\n",
      "        ...,\n",
      "        [ 0.388063,  0.389183, ...,  0.385737,  0.386912],\n",
      "        [ 0.333918,  0.333918, ...,  0.333918,  0.333918]],\n",
      "\n",
      "       [[-0.976046, -1.029718, ..., -0.887624, -0.927455],\n",
      "        [-0.895222, -0.951696, ..., -0.808763, -0.848008],\n",
      "        ...,\n",
      "        [-0.084258, -0.08346 , ..., -0.085797, -0.08504 ],\n",
      "        [-0.05341 , -0.05341 , ..., -0.05341 , -0.05341 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.174698, -0.182278, ..., -0.169951, -0.169541],\n",
      "        [-0.222018, -0.233198, ..., -0.216672, -0.215745],\n",
      "        ...,\n",
      "        [-1.31553 , -1.316041, ..., -1.314459, -1.315002],\n",
      "        [-1.290563, -1.290563, ..., -1.290563, -1.290563]],\n",
      "\n",
      "       [[ 1.738677,  1.775604, ...,  1.66202 ,  1.702176],\n",
      "        [ 1.598069,  1.633223, ...,  1.537579,  1.567922],\n",
      "        ...,\n",
      "        [ 1.464007,  1.461969, ...,  1.46816 ,  1.466072],\n",
      "        [ 1.489238,  1.489238, ...,  1.489238,  1.489238]]])\n",
      "Coordinates:\n",
      "  * lon      (lon) float64 0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 ...\n",
      "  * time     (time) datetime64[ns] 1999-03-16T09:00:00 2000-03-16T09:00:00 ...\n",
      "  * lat      (lat) float64 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 ...\n"
     ]
    }
   ],
   "source": [
    "jfm_sa = (jfm - jfm.mean(dim=('time'))) / (jfm - jfm.mean(dim=('time'))).std(dim=('time'))\n",
    "print(jfm_sa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing the EOF apply a weighting as cosine of the latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71, 360)\n",
      "(18, 71, 360)\n"
     ]
    }
   ],
   "source": [
    "data = jfm_sa.values\n",
    "wgts = np.sqrt(np.cos(np.deg2rad(jfm_sa.coords['lat'].values)).clip(0., 1.))[..., np.newaxis]\n",
    "weights = np.broadcast_arrays(_data[0:1], wgts)[1][0]\n",
    "print(np.shape(_weights))\n",
    "data = data * weights\n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the 10 leading EOF modes. For a basic understanding of EOFs I recommending reading Hannachi's EOF primer [here](http://www.met.rdg.ac.uk/~han/Monitor/eofprimer.pdf) and his paper [here](https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/joc.1499). This code is also adapted from Dawson's [eof package](https://github.com/ajdawson/eofs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71, 360)\n",
      "(18, 25560)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the data to be (time, space)\n",
    "records = len(jfm_sa.coords['time'])\n",
    "originalshape = data.shape[1:]\n",
    "channels = np.prod(originalshape)\n",
    "print(originalshape)\n",
    "#channels = np.prod(_originalshape)\n",
    "#print(channels)\n",
    "data = data.reshape([records, channels])\n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.9 ms, sys: 5.06 ms, total: 48.9 ms\n",
      "Wall time: 27 ms\n",
      "(18, 18)\n",
      "(18,)\n",
      "(18, 25560)\n"
     ]
    }
   ],
   "source": [
    "# Compute the singular value decomposition\n",
    "A, Lh, E = np.linalg.svd(data, full_matrices=False)\n",
    "print(np.shape(A))\n",
    "print(np.shape(Lh))\n",
    "print(np.shape(E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18,)\n",
      "(18, 25560)\n",
      "(18, 18)\n"
     ]
    }
   ],
   "source": [
    "# Construct the eigenvalues and normalize by N-1\n",
    "L = (Lh * Lh) / (float(records - 1))\n",
    "print(np.shape(L))\n",
    "neofs = len(L)\n",
    "\n",
    "# Built in for other examples with missing data\n",
    "flatE = E.copy()\n",
    "print(np.shape(flatE))\n",
    "\n",
    "#print(np.shape(flatE))\n",
    "# Remove the scaling on the principal component time-series that is\n",
    "# implicitily introduced by using SVD instead of eigen-decomposition.\n",
    "# The PCs may be re-scaled later if required.\n",
    "P = A * Lh\n",
    "print(np.shape(P)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the 10 dominant EOFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 10)\n",
      "(10, 71, 360)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Principal Components\n",
    "npcs = 10\n",
    "pcs = P[:, 0:npcs] / np.sqrt(L[0:npcs])\n",
    "print(np.shape(pcs))\n",
    "\n",
    "# Calculate the eofs\n",
    "flat_eofs = flatE[0:npcs, :].copy()\n",
    "eofs = flat_eofs / np.sqrt(L[0:npcs])[:, np.newaxis]\n",
    "# Return the original shape\n",
    "eofs2d = eofs.reshape((npcs,) + originalshape)\n",
    "print(np.shape(eofs2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the EOFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter a start-time an end-time for which to calculate the daily NAO index\n",
    "e.g. lets look at the large negativate NAO of winter 2008/2009:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = '2009-02-01-T12:00:00'\n",
    "etime = '2009-02-14-T12:00:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extend for the winter months (DJF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = '2008-12-01-T12:00:00'\n",
    "etime = '2009-02-28-T12:00:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do for all winters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
