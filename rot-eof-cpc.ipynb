{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Create a daily NAO index using the CPC methodology \n",
    "=====\n",
    "\n",
    "Author: [Ray Bell](https://github.com/raybellwaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load python packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import holoviews as hv\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "hv.notebook_extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the time period of the [SubX](http://iridl.ldeo.columbia.edu/SOURCES/.Models/.SubX/) hindcast: 1999-2016.\n",
    "For a saninty check make sure that 1999-2016 is indeed the hindcast period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_data = xr.open_dataset('http://iridl.ldeo.columbia.edu/SOURCES/.Models'\\\n",
    "                              '/.SubX/.RSMAS/.CCSM4/.hindcast/.zg/dods')\n",
    "print(remote_data.coords['S'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used here is geopotential height at 500 hPa ($Z_{500}$) from ERA-Interim. This data was simply copied from the University of Reading archive. You can however use the [API](https://software.ecmwf.int/wiki/display/CKB/How+to+download+data+via+the+ECMWF+WebAPI) to download the data from ECMWF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two files exists in this directory `ERAI_z500_monthly_1999-2016.nc` (43 Mb) and `b.nc`. `ERAI_z500_monthly_1999-2016.nc` is the monthly mean of $Z_{500}$ January 1996 - December 2016. `b.nc` is daily $Z_{500}$ January 1st 1996 - December 31st 2016. The data has been interpolated to 1° to match the SubX data (360x181) and cut to 20°N-90°N for the analysis. The reference scripts for the data processing beforehand are in the directory `pre_proc`. DO THIS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPC's methodology for the daily NAO index can be found [here](http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/history/method.shtml), with some more information [here](http://www.cpc.ncep.noaa.gov/data/teledoc/telepatcalc.shtml). The procedure is based on [Barston and Livezey (1987)](https://journals.ametsoc.org/doi/pdf/10.1175/1520-0493%281987%29115%3C1083%3ACSAPOL%3E2.0.CO%3B2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My understanding of the methodology is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the region of 20°N-90°N with standardized $Z_{500}$ anomalies.\n",
    "\n",
    "2. Calculate the 10 leading EOFs for each calendar month centred on that month with a window of 3 months. e.g. February is based on January-February-March (JFM). Decisions have to be made for January and December: whether to just use the two-month window or leave out the season. (It shouldn't really matter).\n",
    "\n",
    "3. Rotate the EOFs using the varimax methodology.\n",
    "\n",
    "4. Linearly interpolate the monthly spatial pattern to the day in question. e.g. February 1$^{st}$ will be linearly interpolated from the DJF pattern (think of the pattern defined mid-way through the mid-month: e.g. January 15$^{th}$) and the JFM pattern (February 15$^{th}$).\n",
    "\n",
    "5. Least squared regression approach for daily data? (e.g. last paragraph in the NAO/PNA section [here](http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/history/method.shtml) (I don't understand this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try creating a daily index for one day e.g. February 15$^{th}$ 2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to obtain the NAO pattern from the monthly data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcualte the 10 leading EOFs for all JFMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use xarray's [rolling mean](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.rolling.html) to do the seasonal averages. Then slice it to get all the JFM's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = xr.open_dataarray('ERAI_z500_monthly_1999-2016.nc')\n",
    "print(da)\n",
    "\n",
    "sm = da.rolling(time=3).mean().dropna('time')\n",
    "# Make note that time is now given as the last month in the window e.g. JFM has time 03-16T09 (MM-DDTHH)\n",
    "\n",
    "# Use numpy's slice index to get all the JFM's\n",
    "jfm = sm[0::12,:,:]\n",
    "# Check that all the JFM 1999-2016 were correctly sliced \n",
    "print(jfm.coords['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate standardized anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jfm_sa = (jfm - jfm.mean(dim=('time'))) / (jfm - jfm.mean(dim=('time'))).std(dim=('time'))\n",
    "print(jfm_sa)\n",
    "print(jfm_sa.max())\n",
    "print(jfm_sa.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing the EOF apply a weighting as cosine of the latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jfm_sa.values\n",
    "wgts = np.sqrt(np.cos(np.deg2rad(jfm_sa.coords['lat'].values)).clip(0., 1.))[..., np.newaxis]\n",
    "weights = np.broadcast_arrays(data[0:1], wgts)[1][0]\n",
    "data_weighted = data * weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the 10 leading EOF modes. For a basic understanding of EOFs I recommending reading Hannachi's EOF primer [here](http://www.met.rdg.ac.uk/~han/Monitor/eofprimer.pdf) and his paper [here](https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/joc.1499). There is also a [note](http://www.met.rdg.ac.uk/~sws97mha/Eofs/eof.pdf) written by Ambaum. This code is also adapted from Dawson's [eof package](https://github.com/ajdawson/eofs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to be (time, space)\n",
    "records = len(jfm_sa.coords['time'])\n",
    "originalshape = data_weighted.shape[1:]\n",
    "channels = np.prod(originalshape)\n",
    "data_weighted_flat = data_weighted.reshape([records, channels])\n",
    "print(np.shape(data_weighted_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the singular value decomposition\n",
    "# Principal component, eigenvalue, eof\n",
    "A, Lh, E = np.linalg.svd(data_weighted_flat, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the eigenvalues and normalize by N-1\n",
    "L = (Lh * Lh) / (float(records - 1))\n",
    "\n",
    "# Remove the scaling on the principal component time-series that is\n",
    "# implicitily introduced by using SVD instead of eigen-decomposition.\n",
    "# The PCs may be re-scaled later if required.\n",
    "P = A * Lh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the 10 dominant EOFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Principal Components for a subset and all\n",
    "npcs = 10\n",
    "sub_pcs = P[:, 0:npcs] / np.sqrt(L[0:npcs])\n",
    "pcs = P / np.sqrt(L)\n",
    "print(np.shape(sub_pcs))\n",
    "print(np.shape(pcs))\n",
    "# Put it in a DataArray\n",
    "sub_pcs_da = xr.DataArray(sub_pcs, coords=[jfm_sa.coords['time'], range(sub_pcs.shape[1])],\n",
    "                      dims=['time', 'mode'], name='pcs')\n",
    "\n",
    "# Calculate the eofs for a subset and all\n",
    "sub_flat_eofs = E[0:npcs, :].copy()\n",
    "sub_eofs = sub_flat_eofs / np.sqrt(L[0:npcs])[:, np.newaxis]\n",
    "sub_eofs2d = sub_eofs.reshape((npcs,) + originalshape)\n",
    "eofs = E / np.sqrt(L)[:, np.newaxis]\n",
    "print(np.shape(sub_eofs))\n",
    "#print(np.shape(eofs))\n",
    "print(np.shape(sub_eofs2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test plotting just the 1st eof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sub_eofs2d[0,:,:] * 100 # I can't explain the 100 scaling other than it matches\n",
    "# the calculation of eof by NCL. See eof_5.ncl here https://www.ncl.ucar.edu/Applications/eof.shtml\n",
    "tmp_da = xr.DataArray(tmp, coords=[jfm_sa.coords['lat'], jfm_sa.coords['lon']],\n",
    "                         dims=['lat', 'lon'], name='z500')\n",
    "ax = plt.axes(projection=ccrs.Orthographic(0, 90))\n",
    "ax.coastlines()\n",
    "ax.set_global()\n",
    "tmp_da.plot.contourf(ax=ax, transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the EOFs as covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_out_shape = (npcs,) + originalshape # (10, 71, 361)\n",
    "data_flat = data.reshape([records, channels]) # (18, 25560)\n",
    "\n",
    "# Divisor\n",
    "div = np.float64(sub_pcs.shape[0] - float(records - 1))\n",
    "sub_cov = (np.dot(data_flat.T, sub_pcs).T / div).reshape(sub_out_shape)\n",
    "# Put into xr.DataArray\n",
    "sub_cov_da = xr.DataArray(sub_cov, coords=[range(sub_pcs.shape[1]), jfm_sa.coords['lat'], jfm_sa.coords['lon']],\n",
    "                         dims=['mode', 'lat', 'lon'], name='z500')\n",
    "print(sub_cov_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_cov_da.isel(mode=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the plot nicer using [cartopy](https://github.com/SciTools/cartopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection=ccrs.Orthographic(0, 90))\n",
    "ax.coastlines()\n",
    "ax.set_global()\n",
    "sub_cov_da.isel(mode=0).plot.contourf(ax=ax, transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the variance explained by each mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_varexpl = (L[0:npcs] / L.sum()) * 100\n",
    "print(sub_varexpl)\n",
    "print(np.sum(sub_varexpl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the other modes of variability using [geoviews](https://github.com/ioam/geoviews). Just move the slider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Image [projection=ccrs.Orthographic(0, 90) colorbar=False fig_size=200] (cmap='RdBu_r') Overlay [xaxis=None yaxis=None]\n",
    "dataset = gv.Dataset(sub_cov_da, kdims=['mode', 'lon', 'lat'])\n",
    "dataset.to(gv.Image, ['lon', 'lat']) * gf.coastline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some known patterns here and others may be spurious\n",
    "- 1st Looks a bit like the [Artic Oscillation](http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/loading.html)\n",
    "- 2nd Looks like the [Pacific North American](http://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/pna_loading.html)\n",
    "- 3rd. Is possibly the [NAO](http://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/nao_loading.html) but there is a lot of activity outside of the North Atlantic. Perhaps performing the REOF will reduce the noise for this variable outside of the North Atlantic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For refence I also computed the eofs in NCL to see if they match. The code and plot is in the `NCL` directory and are shown here.\n",
    "![title](NCL/eof.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of rotating the eofs is to maximise the sum of the variances so the coefficients will either be large or near-zero. You can read about it more in [Richman 1986](https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/joc.3370060305) and [Mestas-Nunez, 200](http://www.aoml.noaa.gov/phod/docs/mestas-00.pdf). There some other notes about reofs [here](http://www.jsg.utexas.edu/fu/files/GEO391-W11-REOF.pdf) and some more with math examples [here](https://atmos.washington.edu/~dennis/552_Notes_4.pdf). The most common method is the varimax rotation. There is an answer for how to do with numpy [here](https://stackoverflow.com/questions/17628589/perform-varimax-rotation-in-python-using-numpy), *bmcmenamin* has a fa_kit where is does the rotation [here](https://github.com/bmcmenamin/fa_kit/blob/master/fa_kit/rotation.py) based on a script by *rossfadely* [here](https://github.com/rossfadely/consomme/blob/master/consomme/rotate_factor.py). Dawson also has the rotation as WIP [here](https://github.com/ajdawson/eofs/blob/experimental-rotation/lib/eofs/experimental/rotation/kernels.py). I hope to build on that package. See example 5 using ncl [here](https://www.ncl.ucar.edu/Applications/eof.shtml). The underlying NCL code is [here](https://github.com/yyr/ncl/blob/34bafa4a78ba69ce8852212f59546bb433ce40c6/ni/src/lib/nfpfort/varimax_dp.f) or [here](https://github.com/yyr/ncl/blob/34bafa4a78ba69ce8852212f59546bb433ce40c6/ni/src/lib/nfpfort/varimax_JiangLing_dp.f) with some chat about them [here](https://www.ncl.ucar.edu/Support/talk_archives/2012/3295.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This needs work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-10 # Tolerance value used to determine convergence of the rotation algorithm\n",
    "itermax = 1000 # Maximum number of iterations \n",
    "\n",
    "# Apply kaiser row normalization to sub_eofs\n",
    "scale = np.sqrt((sub_eofs ** 2).sum(axis=0))\n",
    "sub_eofs_norm = sub_eofs / scale\n",
    "\n",
    "rotation = np.eye(npcs, dtype=sub_eofs_norm.dtype) # Initialize\n",
    "delta = 0.\n",
    "for i in range(itermax):\n",
    "    z = np.dot(sub_eofs_norm.T, rotation)\n",
    "    b = np.dot(sub_eofs_norm,\n",
    "               z ** 3 - np.dot(z, np.diag((z ** 2).sum(axis=0)) / channels))\n",
    "    u, s, vh = np.linalg.svd(b)\n",
    "    rotation = np.dot(u, vh)\n",
    "    delta_previous = delta\n",
    "    delta = s.sum()\n",
    "    if delta < delta_previous * (1. + eps): break\n",
    "print(np.shape(rotation))\n",
    "print(np.shape(sub_eofs_norm))\n",
    "reofs = np.dot(sub_eofs_norm.T, rotation).T\n",
    "# de-normalize\n",
    "reofs = reofs * scale\n",
    "print(np.shape(reofs))\n",
    "#print(np.max(sub_eofs))\n",
    "#print(np.min(sub_eofs))\n",
    "#print(np.max(reofs))\n",
    "#print(np.min(reofs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://github.com/yyr/ncl/blob/34bafa4a78ba69ce8852212f59546bb433ce40c6/ni/src/lib/nfpfort/varimax_JiangLing_dp.f\n",
    "\n",
    "# Put the reofs in descending order based on their variance\n",
    "reofs_var = (reofs ** 2).sum(axis=1)\n",
    "print(np.sum(reofs_var))\n",
    "print(reofs_var)\n",
    "reofs_var_des = np.flip(np.sort(reofs_var), axis=0) # There may be a better way to do this\n",
    "print(reofs_var_des)\n",
    "# Get the index of the sort\n",
    "ix = np.flip(np.argsort(reofs_var), axis=0)\n",
    "print(ix)\n",
    "\n",
    "# Index reofs in terms of descending reofs\n",
    "reofs_des = reofs[ix,:]\n",
    "\n",
    "# Reshape to 2d\n",
    "reofs_des2d = reofs_des.reshape((npcs,) + originalshape)\n",
    "print(np.shape(reofs_des2d))\n",
    "\n",
    "# Need to work out how to obtain the new rpcs. Stuck.\n",
    "\n",
    "#reofs2d = reofs.reshape((npcs,) + originalshape)\n",
    "#print(np.shape(reofs2d))\n",
    "#print(np.max(reofs2d))\n",
    "#print(np.min(reofs2d))\n",
    "\n",
    "#(L[0:npcs] / L.sum()) * 100\n",
    "# Put them in desending order\n",
    "\n",
    "# Calculate the new variance by the rotated modes\n",
    "# Compute eigen values\n",
    "#eof_ev = L[0:npcs]\n",
    "#eof_vf = L[0:npcs] / L.sum()\n",
    "#ratio = eof_vf[0] / eof_ev[0]\n",
    "#vf_reofs = reofs_var * ratio\n",
    "#vf_reofs = np.array([1. / float(reofs.shape[1])] * 10)\n",
    "#print(vf_reofs)\n",
    "\n",
    "#nspace = np.prod(channels)\n",
    "#ev = eofs.reshape([npcs, nspace])\n",
    "#print(np.shape(ev))\n",
    "#field = eofs.reshape((npcs,) + originalshape)\n",
    "#print(np.shape(field))\n",
    "#rpcs = np.dot(ev, reofs.T)\n",
    "# Should be (time(18), mode(10)) but it's (10, 10) eofs oroginally is 18 x channels (this svd computes eofs as the len of time)\n",
    "# However, eofs is cut to 10 to include the 10 dominant ones beforehand so returns (10, 10)\n",
    "#print(np.shape(rpcs))\n",
    "#print(rpcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reof\n",
    "tmp = reofs_des2d[0,:,:] * 100 # I can't explain the 100 scaling other than it matches\n",
    "# the calculation of eof by NCL. See eof_5.ncl here https://www.ncl.ucar.edu/Applications/eof.shtml\n",
    "tmp_da = xr.DataArray(tmp, coords=[jfm_sa.coords['lat'], jfm_sa.coords['lon']],\n",
    "                         dims=['lat', 'lon'], name='z500')\n",
    "ax = plt.axes(projection=ccrs.Orthographic(0, 90))\n",
    "ax.coastlines()\n",
    "ax.set_global()\n",
    "tmp_da.plot.contourf(ax=ax, transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look right..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For refence I also computed the rotated eofs in NCL to see if they match. The code and plot is in the `NCL` directory and are shown here.\n",
    "![title](NCL/rot_eof.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter a start-time an end-time for which to calculate the daily NAO index\n",
    "e.g. lets look at the large negativate NAO of winter 2008/2009:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = '2009-02-01-T12:00:00'\n",
    "etime = '2009-02-14-T12:00:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extend for the winter months (DJF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = '2008-12-01-T12:00:00'\n",
    "etime = '2009-02-28-T12:00:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do for all winters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
